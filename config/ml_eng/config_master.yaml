MODEL: unsloth/Llama-3-8b-bnb-4bit
MAX_SEQ_LENGTH: 16384
TRAINING_PRECISION: BF16
QUANT_NF4_LAYERS: ["gate_proj", "up_proj", "down_proj"]
QUANT_FP4_LAYERS: ["q_proj", "k_proj", "v_proj"]
CHECKPOINT_LAYERS: ["attention"]
ADAPTER_TYPE: DORA
RANK_Q_V: 96
RANK_K_O: 64
RANK_MLP: 32
OPTIMIZER_CHOICE: "PagedAdamW"
LR_MAGNITUDE: 6.5e-5
LR_DIRECTION: 1.5e-5
LR_WARMUP_RATIO: 0.03
LR_SETTLING_TAIL_RATIO: 0.03
CURRICULUM_EASY_RATIO: 0.30
CURRICULUM_MEDIUM_RATIO: 0.50
CURRICULUM_HARD_RATIO: 0.20
NEFTUNE_NOISE_TARGET: 5.0
NOISE_RAMP_RATIO: 0.50
MIXOUT_LAYERS: ["attention"]
DRAFT_MODEL_ID: "unsloth/Llama-3-8b-4bit-distilled"
VERIFIER_MODEL_ID: "fused_apex_model"
FUSION_DELTA_NORM_MAX: 2.0
INITIAL_ADAPTER_NORM_CLAMP: 0.03
