{
  "AID": "deployment/serving_manifest.json",
  "Proof_ID": "PRF-SERVING-004",
  "Purpose": "Specification for high-throughput, speculative-decoded serving.",
  
  "engine_type": "vLLM",
  "engine_version_min": "0.4.0",
  "inference_mode": "speculative_decoding",
  
  "verifier": {
    "model_id": "fused_apex_model",
    "quantization": "BF16",
    "gpu_memory_strategy": "Paged_KV_Cache",
    "batching_strategy": "Continuous_Batching"
  },
  
  "draft_model": {
    "model_id": "unsloth/Llama-3-8b-4bit-distilled",
    "tokenizer_id": "fused_apex_model",
    "quantization": "INT8"
  },
  
  "speculative_decoding_config": {
    "initial_horizon_tokens": 5,
    "adaptive_horizon_loop": true,
    "verifier_accept_rate_metric": "VAR",
    "VAR_increase_threshold": 0.8,
    "VAR_decrease_threshold": 0.6,
    "increase_step": 1,
    "decrease_step": 1
  },
  
  "safety_checks_required_at_runtime": [
    "Fusion_Delta_Norm_Check",
    "Tokenizer_Hash_Check"
  ]
}
